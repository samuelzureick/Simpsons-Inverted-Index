{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWW5mCRe10Yd",
        "outputId": "a7e3807d-692f-4ee8-b311-4687b2e65f6a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME5ws8uY1mhu",
        "outputId": "4607fe92-b9c4-444a-b0b2-c647a1cd4a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import pandas\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "\n",
        "class InvertedIndex:\n",
        "    \"\"\"\n",
        "    Construct Inverted Index\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "      self.inverted_index = None\n",
        "      self.simpsonsTerms = None\n",
        "      self.docIDS = None\n",
        "        \n",
        "        \n",
        "    def read_data(self, path: str) -> list:\n",
        "      \"\"\"\n",
        "      Read files from a directory and then append the data of each file into a list.\n",
        "      \"\"\"\n",
        "      t = []\n",
        "      self.simpsonsTerms = []\n",
        "      self.docIDS = []\n",
        "\n",
        "      for f in os.listdir(path):\n",
        "        # if this is a text file\n",
        "        if f[len(f)-3:] == \"txt\" and f != \"examples.txt\":\n",
        "          file = open(path+\"/\"+f, 'r')\n",
        "          # keep track of docID\n",
        "          self.docIDS.append(f[:len(f)-4])\n",
        "          txt = file.read()\n",
        "\n",
        "          # trim document to everyting after external links\n",
        "          l=txt.find(\"External links\")\n",
        "          if l == -1:\n",
        "            # sometimes spelled differently\n",
        "            l = txt.find(\"external links\")\n",
        "          txt = txt[l+len(\"External Links\"):]\n",
        "          t.append(txt)\n",
        "          file.close()\n",
        "        elif f[len(f)-3:] == \"csv\": # simpsons characters and locations\n",
        "          self.simpsonsTerms.append(pandas.read_csv(path+\"/\"+f)[\"name\"].tolist())\n",
        "      for i in range(len(t)):\n",
        "        # preprocess all of the wikipedia text\n",
        "        t[i] = self.process_document(t[i])\n",
        "      return t\n",
        "\n",
        "    def process_document(self, document: str) -> list:\n",
        "        \"\"\"\n",
        "        pre-process a document and return a list of its terms\n",
        "        str->list\"\"\"\n",
        "        #tokenize \n",
        "        document = wordpunct_tokenize(document)\n",
        "        # construct dictionary of stopwords for O(1) lookup time when removing stopwords\n",
        "        stops = Counter(stopwords.words())\n",
        "        # list comp to casefold and remove non alpha chars from doc\n",
        "        document = [re.sub(\"\\W\", \"\", w.lower()) for w in document if re.sub(\"\\W\", \"\", w.lower()) not in stops]\n",
        "        return document\n",
        "\n",
        "    \n",
        "    def index_corpus(self, documents: list) -> None:\n",
        "        \"\"\"\n",
        "        index given documents\n",
        "        list->None\"\"\"\n",
        "        # start clock\n",
        "        start = time.time()\n",
        "\n",
        "        \"\"\"\n",
        "        generate tokens for terms that are in the lists of characters and locations for simpsons\n",
        "        have to normalize terms first usings regex and case folding\n",
        "        as simpsons specific terms are sometimes multiple words, the MWE tokenizer flattens these to \n",
        "        single word tokens seperated by underscores, allowing us to continue the processing as we would \n",
        "        for single word tokenization\n",
        "        \"\"\"\n",
        "        self.simpsonsTerms = [re.sub(r'[^A-Za-z0-9 ]+', \"\", t.lower()).split() for d in self.simpsonsTerms for t in d]\n",
        "        tokenizer = MWETokenizer(self.simpsonsTerms)\n",
        "\n",
        "        #construct list of tokens for the document\n",
        "        doc_tokens = []\n",
        "        for c,v in enumerate(documents):\n",
        "          tokens = tokenizer.tokenize(v)\n",
        "          # construct token list with each element in form [<token>, (<docID>, <position>)]\n",
        "          tokens = [[token, (self.docIDS[c], count)] for count, token in enumerate(tokens) if token != \"\"]\n",
        "          doc_tokens.append(tokens)\n",
        "\n",
        "        # flatten list of tokens from a 2d list to a 1d list\n",
        "        doc_tokens = [t for d in doc_tokens for t in d]\n",
        "        \"\"\"\n",
        "        sort document tokens first by token alphabetically, then by document ID\n",
        "        have to sort twice because 3.2 should come before 3.11 as the docID after the\n",
        "        stop indicates episode number, so 3.2 < 3.11 by this ordering\n",
        "        \"\"\"\n",
        "        doc_tokens.sort(key=lambda doc: (doc[0], int(doc[1][0][2:])))\n",
        "        doc_tokens.sort(key=lambda doc: (doc[0], int(doc[1][0][0])))\n",
        "\n",
        "        # make seperate copy of tokens to calculate document freq later\n",
        "        doc_copy = doc_tokens.copy()\n",
        "        \n",
        "        # temp set to get list of unique index terms\n",
        "        doc_set = list(dict.fromkeys([t[0] for t in doc_tokens]))\n",
        "        self.inverted_index = []\n",
        "        \"\"\"\n",
        "        iterate through each unique index term\n",
        "        append to inverted index [<token>, [<docID, position>, ...]]\n",
        "        shrink size of tokens each time for O(1) initial lookup-\n",
        "        start at beginning of list each iteration, and go through tokens\n",
        "        iteratively, adding to postings list, until the token changes.\n",
        "        remove those processed tokens from token list, and start again from \n",
        "        beginning with new token\n",
        "        \"\"\"\n",
        "        for i in range(len(doc_set)):\n",
        "          self.inverted_index.append([doc_set[i],[doc_tokens[0][1]]])\n",
        "          j = 1\n",
        "          try:\n",
        "            while doc_tokens[j][0] == self.inverted_index[i][0]:\n",
        "              self.inverted_index[i][1].append(doc_tokens[j][1])          \n",
        "              j += 1\n",
        "            doc_tokens = doc_tokens[j:]\n",
        "          except:\n",
        "            pass\n",
        "        # remove duplicate DOCIDS from postings list in document copy\n",
        "        doc_copy = [[d[0], d[1][0]]for d in doc_copy]\n",
        "        doc_copy = sorted(list(set([tuple(element) for element in doc_copy])))\n",
        "        \"\"\"\n",
        "        calculate document frequency\n",
        "        for each token in the inverted index, count each docID in\n",
        "        postings list only once, and after inserting DF in inverted index\n",
        "        try and except block for the final value in tokens list\n",
        "        \"\"\"\n",
        "        for i in range(len(self.inverted_index)):\n",
        "          count = 1\n",
        "          value = doc_copy[0][0]\n",
        "          j = 1\n",
        "          try:\n",
        "            while doc_copy[j][0] == doc_copy[0][0]:\n",
        "              count += 1\n",
        "              j += 1\n",
        "          except:\n",
        "            self.inverted_index[i].insert(1,count)\n",
        "            continue\n",
        "          doc_copy = doc_copy[j:]\n",
        "          self.inverted_index[i].insert(1,count)\n",
        "\n",
        "\n",
        "        # construct dictionary from current inverted index\n",
        "        # structure: key: <token>, value: [ <DF>, [(<docID>, <position>), ... ] ]\n",
        "        temp_dict = {}\n",
        "        for term in self.inverted_index:\n",
        "          temp_dict[term[0]] = [term[1], term[2]]\n",
        "        self.inverted_index = temp_dict\n",
        "\n",
        "        # stop time\n",
        "        total_time = time.time() - start\n",
        "        print(\"Total time to construct inverted index: \" +str(total_time) + \" seconds.\")\n",
        "\n",
        "        # calculate size of postings list\n",
        "        items = 0\n",
        "        for key in self.inverted_index:\n",
        "          items += len(self.inverted_index[key][1])\n",
        "\n",
        "        avg_len = items/len(self.inverted_index)\n",
        "\n",
        "        print(\"Size of inverted index: \\nunique terms indexed: \" + str(len(self.inverted_index)))\n",
        "        print(\"average postings list length: \" +str(avg_len) + \" postings\")\n",
        "        print(\"total unique positional indicies: \" +str(items))\n",
        "                         \n",
        "    def dump(self, path: str, verbose = False) -> None:\n",
        "        \"\"\"\n",
        "        provide a dump function to show index entries for a given set of terms        \n",
        "        \"\"\"\n",
        "        # initialize multi word tokenizer for special simpsons terms\n",
        "        tokenizer = MWETokenizer(self.simpsonsTerms)\n",
        "\n",
        "        # read example terms and normalize them\n",
        "        f = open(path, \"r\")\n",
        "        examples = f.readlines()\n",
        "        tokens = [example.lower().split() for example in examples]\n",
        "\n",
        "        # tokenize the term, for each token search for matches in corpus\n",
        "        for t in tokens:\n",
        "          t = tokenizer.tokenize(t)\n",
        "          for sub in t:\n",
        "            print(\"*********************\")\n",
        "            print(\"Token: \" +sub)\n",
        "            try:\n",
        "              print(\"Found in \" + str(self.inverted_index[sub][0]) + \" documents\")\n",
        "              if verbose == True:\n",
        "                for post in self.inverted_index[sub][1]:\n",
        "                  # if verbose is true, indicate each document and position for each finding\n",
        "                  print(\"docID: \" +post[0] +\" @ position \" +str(post[1]))\n",
        "                print()\n",
        "              else:\n",
        "                # list each ID only once\n",
        "                ids = list(dict.fromkeys([post[0] for post in self.inverted_index[sub][1]]))\n",
        "                print(\"Found in documents: \")\n",
        "                for id in ids:\n",
        "                  print(id)\n",
        "            except:\n",
        "              print(\"TOKEN NOT FOUND IN INDEX\\n\")\n",
        "\n",
        "       \n",
        "    def proximity_search(self, term1: str, term2: str, window_size: int) -> dict:\n",
        "        \"\"\"\n",
        "        This is Task 2\"\"\"\n",
        "        #normalize input serach terms\n",
        "        term1, term2 = re.sub(r'[^A-Za-z0-9 _]+', \"\", term1.lower()), re.sub(r'[^A-Za-z0-9 _]+', \"\", term2.lower())\n",
        "\n",
        "        # get postings list\n",
        "        term1_postings = self.inverted_index[term1]\n",
        "        term2_postings = self.inverted_index[term2]\n",
        "\n",
        "        #only search if both terms appear in corpus\n",
        "        if term1_postings[0] != 0 and term2_postings[0] != 0:\n",
        "          # remove DF for each postings list\n",
        "          term1_postings, term2_postings = term1_postings[1], term2_postings[1]\n",
        "          # and merge two lists based on docID\n",
        "          term1_postings = [post for post in term1_postings if post[0] in [d[0] for d in term2_postings]]\n",
        "          term2_postings = [post for post in term2_postings if post[0] in [d[0] for d in term1_postings]]\n",
        "          # compute matches within window using list comprehension\n",
        "          matches = [[p1,p2] for p1 in term1_postings for p2 in term2_postings if p1[0] == p2[0] and ((p1[1] in range(p2[1]-window_size, p2[1] + window_size) or (p2[1] in range(p1[1]-window_size, p1[1] + window_size))))]\n",
        "          # construct dicitonary to return\n",
        "          match_dict = {}\n",
        "          \n",
        "          for match in matches:\n",
        "            match_dict[match[0][0]] = [match[0][1], match[1][1]]\n",
        "          \n",
        "          return match_dict\n",
        "        # return empty dict if no matches found\n",
        "        print(\"no matches found within windows for search terms!\")\n",
        "        return {}\n",
        "\n",
        "        \n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQV_Vhbe1mhx",
        "outputId": "bc8cc82b-ccc4-4389-b95d-1fd1e4349f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time to construct inverted index: 15.814880609512329 seconds.\n",
            "Size of inverted index: \n",
            "unique terms indexed: 13352\n",
            "average postings list length: 6.995206710605153 postings\n",
            "total unique positional indicies: 93400\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"main call function\"\n",
        "    index = InvertedIndex() # initilaise the index\n",
        "    corpus = index.read_data('/content/drive/MyDrive/Colab Notebooks/Simpsons2022') # specify the directory path in which files are located\n",
        "    index.index_corpus(corpus) # index documents/corpus\n",
        "    \n",
        "    return index\n",
        "    \n",
        "index = main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test dump functionality\n",
        "index.dump(\"/content/drive/MyDrive/Colab Notebooks/Simpsons2022/examples.txt\")\n",
        "\n",
        "# perform proximity search on lisa and bart simpson with a window of 20 \n",
        "matches = index.proximity_search(\"lisa\",\"bart_simpson\",20)\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1lMtznOAcbO",
        "outputId": "e811843c-917b-4307-8b34-1a7c5ddefb48"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*********************\n",
            "Token: bart\n",
            "Found in 104 documents\n",
            "Found in documents: \n",
            "3.1\n",
            "3.2\n",
            "3.3\n",
            "3.4\n",
            "3.5\n",
            "3.6\n",
            "3.7\n",
            "3.8\n",
            "3.9\n",
            "3.10\n",
            "3.11\n",
            "3.12\n",
            "3.13\n",
            "3.14\n",
            "3.15\n",
            "3.16\n",
            "3.18\n",
            "3.19\n",
            "3.20\n",
            "3.21\n",
            "3.22\n",
            "3.23\n",
            "3.24\n",
            "4.1\n",
            "4.5\n",
            "4.6\n",
            "4.7\n",
            "4.8\n",
            "4.9\n",
            "4.10\n",
            "4.11\n",
            "4.12\n",
            "4.13\n",
            "4.14\n",
            "4.15\n",
            "4.16\n",
            "4.18\n",
            "4.19\n",
            "4.20\n",
            "4.21\n",
            "4.22\n",
            "5.1\n",
            "5.2\n",
            "5.3\n",
            "5.4\n",
            "5.5\n",
            "5.6\n",
            "5.7\n",
            "5.8\n",
            "5.9\n",
            "5.10\n",
            "5.11\n",
            "5.12\n",
            "5.13\n",
            "5.14\n",
            "5.15\n",
            "5.16\n",
            "5.17\n",
            "5.18\n",
            "5.19\n",
            "5.20\n",
            "5.21\n",
            "5.22\n",
            "6.1\n",
            "6.2\n",
            "6.3\n",
            "6.4\n",
            "6.5\n",
            "6.6\n",
            "6.7\n",
            "6.8\n",
            "6.9\n",
            "6.10\n",
            "6.14\n",
            "6.16\n",
            "6.17\n",
            "6.18\n",
            "6.19\n",
            "6.20\n",
            "6.21\n",
            "6.22\n",
            "6.23\n",
            "6.24\n",
            "6.25\n",
            "7.1\n",
            "7.2\n",
            "7.3\n",
            "7.4\n",
            "7.5\n",
            "7.6\n",
            "7.7\n",
            "7.8\n",
            "7.9\n",
            "7.10\n",
            "7.11\n",
            "7.12\n",
            "7.13\n",
            "7.15\n",
            "7.18\n",
            "7.20\n",
            "7.21\n",
            "7.22\n",
            "7.24\n",
            "7.25\n",
            "*********************\n",
            "Token: first\n",
            "TOKEN NOT FOUND IN INDEX\n",
            "\n",
            "*********************\n",
            "Token: image\n",
            "Found in 9 documents\n",
            "Found in documents: \n",
            "3.7\n",
            "3.9\n",
            "3.16\n",
            "3.18\n",
            "5.13\n",
            "7.2\n",
            "7.5\n",
            "7.21\n",
            "7.25\n",
            "*********************\n",
            "Token: montage\n",
            "Found in 7 documents\n",
            "Found in documents: \n",
            "3.16\n",
            "3.21\n",
            "4.4\n",
            "4.18\n",
            "6.3\n",
            "7.3\n",
            "7.10\n",
            "*********************\n",
            "Token: well\n",
            "TOKEN NOT FOUND IN INDEX\n",
            "\n",
            "*********************\n",
            "Token: top\n",
            "Found in 51 documents\n",
            "Found in documents: \n",
            "3.1\n",
            "3.5\n",
            "3.6\n",
            "3.7\n",
            "3.8\n",
            "3.10\n",
            "3.16\n",
            "3.17\n",
            "3.23\n",
            "4.1\n",
            "4.2\n",
            "4.3\n",
            "4.4\n",
            "4.6\n",
            "4.8\n",
            "4.9\n",
            "4.12\n",
            "4.15\n",
            "4.16\n",
            "4.17\n",
            "4.22\n",
            "5.1\n",
            "5.3\n",
            "5.4\n",
            "5.7\n",
            "5.9\n",
            "5.13\n",
            "5.15\n",
            "5.18\n",
            "5.19\n",
            "5.22\n",
            "6.2\n",
            "6.4\n",
            "6.6\n",
            "6.9\n",
            "6.14\n",
            "6.16\n",
            "6.19\n",
            "6.20\n",
            "6.21\n",
            "6.23\n",
            "6.24\n",
            "6.25\n",
            "7.1\n",
            "7.4\n",
            "7.6\n",
            "7.7\n",
            "7.18\n",
            "7.19\n",
            "7.21\n",
            "7.24\n",
            "*********************\n",
            "Token: arguably\n",
            "Found in 3 documents\n",
            "Found in documents: \n",
            "3.16\n",
            "4.12\n",
            "5.9\n",
            "*********************\n",
            "Token: best\n",
            "TOKEN NOT FOUND IN INDEX\n",
            "\n",
            "*********************\n",
            "Token: number\n",
            "Found in 46 documents\n",
            "Found in documents: \n",
            "3.1\n",
            "3.7\n",
            "3.9\n",
            "3.11\n",
            "3.16\n",
            "3.17\n",
            "3.18\n",
            "3.19\n",
            "3.21\n",
            "4.1\n",
            "4.2\n",
            "4.7\n",
            "4.9\n",
            "4.11\n",
            "4.12\n",
            "4.15\n",
            "4.16\n",
            "4.17\n",
            "4.21\n",
            "4.22\n",
            "5.1\n",
            "5.2\n",
            "5.3\n",
            "5.8\n",
            "5.10\n",
            "5.11\n",
            "5.13\n",
            "5.14\n",
            "5.21\n",
            "5.22\n",
            "6.2\n",
            "6.4\n",
            "6.6\n",
            "6.12\n",
            "6.20\n",
            "6.22\n",
            "6.24\n",
            "6.25\n",
            "7.1\n",
            "7.7\n",
            "7.8\n",
            "7.10\n",
            "7.13\n",
            "7.14\n",
            "7.21\n",
            "7.23\n",
            "*********************\n",
            "Token: humor\n",
            "Found in 22 documents\n",
            "Found in documents: \n",
            "3.1\n",
            "3.3\n",
            "3.7\n",
            "3.10\n",
            "3.13\n",
            "3.15\n",
            "3.16\n",
            "3.20\n",
            "4.19\n",
            "5.1\n",
            "5.10\n",
            "5.11\n",
            "5.12\n",
            "6.6\n",
            "6.10\n",
            "6.14\n",
            "6.16\n",
            "7.4\n",
            "7.5\n",
            "7.10\n",
            "7.14\n",
            "7.18\n",
            "*********************\n",
            "Token: dollarydoos\n",
            "Found in 1 documents\n",
            "Found in documents: \n",
            "6.16\n",
            "*********************\n",
            "Token: bart_simpson\n",
            "Found in 15 documents\n",
            "Found in documents: \n",
            "3.4\n",
            "3.13\n",
            "4.1\n",
            "4.2\n",
            "4.7\n",
            "5.3\n",
            "5.5\n",
            "5.15\n",
            "6.5\n",
            "6.7\n",
            "6.18\n",
            "6.23\n",
            "6.25\n",
            "7.1\n",
            "7.4\n",
            "*********************\n",
            "Token: gordie\n",
            "Found in 1 documents\n",
            "Found in documents: \n",
            "3.16\n",
            "*********************\n",
            "Token: howe\n",
            "Found in 2 documents\n",
            "Found in documents: \n",
            "3.2\n",
            "3.16\n",
            "*********************\n",
            "Token: recalled\n",
            "Found in 6 documents\n",
            "Found in documents: \n",
            "3.11\n",
            "5.16\n",
            "6.2\n",
            "7.2\n",
            "7.4\n",
            "7.25\n",
            "*********************\n",
            "Token: bart\n",
            "Found in 104 documents\n",
            "Found in documents: \n",
            "3.1\n",
            "3.2\n",
            "3.3\n",
            "3.4\n",
            "3.5\n",
            "3.6\n",
            "3.7\n",
            "3.8\n",
            "3.9\n",
            "3.10\n",
            "3.11\n",
            "3.12\n",
            "3.13\n",
            "3.14\n",
            "3.15\n",
            "3.16\n",
            "3.18\n",
            "3.19\n",
            "3.20\n",
            "3.21\n",
            "3.22\n",
            "3.23\n",
            "3.24\n",
            "4.1\n",
            "4.5\n",
            "4.6\n",
            "4.7\n",
            "4.8\n",
            "4.9\n",
            "4.10\n",
            "4.11\n",
            "4.12\n",
            "4.13\n",
            "4.14\n",
            "4.15\n",
            "4.16\n",
            "4.18\n",
            "4.19\n",
            "4.20\n",
            "4.21\n",
            "4.22\n",
            "5.1\n",
            "5.2\n",
            "5.3\n",
            "5.4\n",
            "5.5\n",
            "5.6\n",
            "5.7\n",
            "5.8\n",
            "5.9\n",
            "5.10\n",
            "5.11\n",
            "5.12\n",
            "5.13\n",
            "5.14\n",
            "5.15\n",
            "5.16\n",
            "5.17\n",
            "5.18\n",
            "5.19\n",
            "5.20\n",
            "5.21\n",
            "5.22\n",
            "6.1\n",
            "6.2\n",
            "6.3\n",
            "6.4\n",
            "6.5\n",
            "6.6\n",
            "6.7\n",
            "6.8\n",
            "6.9\n",
            "6.10\n",
            "6.14\n",
            "6.16\n",
            "6.17\n",
            "6.18\n",
            "6.19\n",
            "6.20\n",
            "6.21\n",
            "6.22\n",
            "6.23\n",
            "6.24\n",
            "6.25\n",
            "7.1\n",
            "7.2\n",
            "7.3\n",
            "7.4\n",
            "7.5\n",
            "7.6\n",
            "7.7\n",
            "7.8\n",
            "7.9\n",
            "7.10\n",
            "7.11\n",
            "7.12\n",
            "7.13\n",
            "7.15\n",
            "7.18\n",
            "7.20\n",
            "7.21\n",
            "7.22\n",
            "7.24\n",
            "7.25\n",
            "*********************\n",
            "Token: the\n",
            "TOKEN NOT FOUND IN INDEX\n",
            "\n",
            "*********************\n",
            "Token: lover\n",
            "Found in 3 documents\n",
            "Found in documents: \n",
            "3.16\n",
            "5.21\n",
            "6.3\n",
            "*********************\n",
            "Token: cents\n",
            "Found in 6 documents\n",
            "Found in documents: \n",
            "3.11\n",
            "3.16\n",
            "4.15\n",
            "5.1\n",
            "5.6\n",
            "7.4\n",
            "*********************\n",
            "Token: won\n",
            "TOKEN NOT FOUND IN INDEX\n",
            "\n",
            "*********************\n",
            "Token: voice-overs\n",
            "TOKEN NOT FOUND IN INDEX\n",
            "\n",
            "*********************\n",
            "Token: simpsonovi\n",
            "TOKEN NOT FOUND IN INDEX\n",
            "\n",
            "{'3.13': [112, 129], '7.4': [145, 137]}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}